{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600132053014",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import tensorflow as tf\n",
    "\n",
    "## Save and load\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "### Load data and word2vec vectors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train & valid splits\n",
    "f = open('../data/preprocessed.pkl','rb')\n",
    "train, valid = pickle.load(f)\n",
    "labels = train.columns[2:]\n",
    "# Get Ys\n",
    "y_train = train[labels].values\n",
    "y_valid = valid[labels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "word 3000000 has a vector length of 1\nword 0.3828125 has a vector length of 299\nword 0.049804688 has a vector length of 299\nword 0.037597656 has a vector length of 299\nExtracted 2999996 word vectors\n"
    }
   ],
   "source": [
    "## BUILD WORD2VEC EMBEDDINGS DICTIONARY\n",
    "embeddings_dict = dict()\n",
    "f = open(r'../data/GoogleNews-vectors-negative300.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    if len(vec) == 300: \n",
    "        embeddings_dict[word] = vec\n",
    "    else:\n",
    "        print('word {0} has a vector length of {1}'.format(word,len(vec))) # got some problematic vectors\n",
    "f.close()\n",
    "print('Extracted {} word vectors'.format(len(embeddings_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional params\n",
    "embed_size = 300 # length of vector representation for each word\n",
    "max_features = 50000 # # of unique words to use (number of rows in embedding vector)\n",
    "max_len = 100 # # of words in a comment to use"
   ]
  },
  {
   "source": [
    "### Standard keras preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into list of strings\n",
    "train_list = list(train['comment_text'].values)\n",
    "valid_list = list(valid['comment_text'].values)\n",
    "# Create and fit tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_list)\n",
    "# Tokenize train and valid\n",
    "train_tokenized = tokenizer.texts_to_sequences(train_list)\n",
    "valid_tokenized = tokenizer.texts_to_sequences(valid_list)\n",
    "# Padd\n",
    "X_train = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "X_valid = pad_sequences(valid_tokenized, maxlen=max_len)"
   ]
  },
  {
   "source": [
    "### Use w2c vectors to create embedding matrix. If words are not in w2c dictionary, then use random initialization. To generate random initialization, use the same mean and std as w2c vectors "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get mean and std from w2c vectors\n",
    "w2c_mean, w2c_std = np.stack(embeddings_dict.values()).mean(), np.stack(embeddings_dict.values()).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordidx_dict = tokenizer.word_index # get word indices\n",
    "num_words = min(len(wordidx_dict), max_features) # should be max 50000, but could be less if wordidx dict contains <50000 words\n",
    "# random initliaization of weights\n",
    "embedding_matrix = np.random.normal(w2c_mean, \n",
    "                                    w2c_std, \n",
    "                                    size = (num_words, embed_size)) \n",
    "# update embedding matrix with w2c vectors\n",
    "for word, idx in wordidx_dict.items():\n",
    "    if idx < max_features: # stay within max # of features\n",
    "        # grab w2c vector if exists\n",
    "        vec = embeddings_dict.get(word) \n",
    "        # if w2c vector exists, add to embedding matrix (i.e., replace random initialization)\n",
    "        if vec is not None: embedding_matrix[idx] = vec "
   ]
  },
  {
   "source": [
    "### Build RNN\n",
    "- 1 bidirectional LSTM layer with 2 FC layers and dropouts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(max_len,))\n",
    "x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(input) # embedding layer to obtain vectors for words\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x) # bidirectional lstm layer\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x) # 1st FC layer\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x) # Output label (6 outputs, 1 for each class for multi-label classification)\n",
    "model = Model(inputs=input, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "### Fit model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/2\n3007/3007 [==============================] - 1087s 362ms/step - loss: 0.0625 - accuracy: 0.9091 - val_loss: 0.0503 - val_accuracy: 0.9943\nEpoch 2/2\n3007/3007 [==============================] - 1177s 392ms/step - loss: 0.0456 - accuracy: 0.9780 - val_loss: 0.0468 - val_accuracy: 0.9933\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x12a40b9b488>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=32, \n",
    "          epochs=2,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\lawre\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: ../artifacts/w2c_lstm\\assets\n"
    }
   ],
   "source": [
    "## Save model\n",
    "model.save('../artifacts/w2c_lstm')"
   ]
  },
  {
   "source": [
    "### Check performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../artifacts/w2c_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get predictions\n",
    "preds = model.predict(X_valid, batch_size=32)\n",
    "preds_t = preds\n",
    "preds_t[preds<=0.5] = 0\n",
    "preds_t[preds>0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Results for toxic comments: Accuracy - 0.96; Precision - 0.83; Recall - 0.78; F1 - 0.80\nResults for severe_toxic comments: Accuracy - 0.99; Precision - 0.48; Recall - 0.41; F1 - 0.44\nResults for obscene comments: Accuracy - 0.98; Precision - 0.84; Recall - 0.82; F1 - 0.83\nResults for threat comments: Accuracy - 1.00; Precision - 0.00; Recall - 0.00; F1 - 0.00\nResults for insult comments: Accuracy - 0.97; Precision - 0.77; Recall - 0.67; F1 - 0.72\nResults for identity_hate comments: Accuracy - 0.99; Precision - 0.77; Recall - 0.19; F1 - 0.30\n"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Label','Accuracy', 'Recall', 'Precision', 'F1', 'Vectorizer', 'model'])\n",
    "\n",
    "## Print results\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "for i in range(preds_t.shape[1]):\n",
    "    i_preds = preds_t[:,i]\n",
    "    i_true = y_valid[:,i]\n",
    "    label = labels[i]\n",
    "\n",
    "    # Evaluate predictions\n",
    "    acc, prec, recall, f1 = (accuracy_score(i_true, i_preds), \n",
    "                            precision_score(i_true, i_preds), \n",
    "                            recall_score(i_true, i_preds), \n",
    "                            f1_score(i_true, i_preds))\n",
    "    \n",
    "    # Save results to dataframe\n",
    "    results = results.append({'Label': label,\n",
    "                            'Accuracy':acc,\n",
    "                            'Recall':recall,\n",
    "                            'Precision':prec,\n",
    "                            'F1':f1,\n",
    "                            'Vectorizer':'w2c',\n",
    "                            'model': 'lstm'}, \n",
    "                            ignore_index = True)\n",
    "    \n",
    "    # print results\n",
    "    print('Results for {0} comments: Accuracy - {1:.2f}; Precision - {2:.2f}; Recall - {3:.2f}; F1 - {4:.2f}'.format(\n",
    "                                    label, \n",
    "                                    acc, \n",
    "                                    prec, \n",
    "                                    recall,\n",
    "                                    f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE RESULTS\n",
    "results.to_csv('../artifacts/w2c_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}