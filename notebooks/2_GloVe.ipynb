{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA MANIPULATION\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "## CLASSIFICATION\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA AND GET LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/preprocessed.pkl','rb')\n",
    "train, valid = pickle.load(f)\n",
    "labels = train.columns[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Extracted 400000 word vectors\n"
    }
   ],
   "source": [
    "## BUILD GLOVE EMBEDDINGS DICTIONARY\n",
    "embeddings_dict = dict()\n",
    "f = open(r'../data/glove.6B.300d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_dict[word] = vec\n",
    "f.close()\n",
    "print('Extracted {} word vectors'.format(len(embeddings_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SKLEARN COMPATIBLE GLOVE VECTORIZER TRANSFORMER \n",
    "class gloveVectorizer(object):\n",
    "    def __init__(self, embeddings_dict):\n",
    "        self.embeddings_dict = embeddings_dict\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(embeddings_dict[next(iter(embeddings_dict))])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def sentence2vec(self, s):\n",
    "        '''\n",
    "        Input:\n",
    "        Sentence string\n",
    "\n",
    "        Transformations:\n",
    "        Get vector for each word -> Average vectors\n",
    "\n",
    "        Output:\n",
    "        Vector for sentence\n",
    "        '''\n",
    "        words = s.split()\n",
    "        M = []\n",
    "        for w in words:\n",
    "            try:\n",
    "                M.append(self.embeddings_dict[w])\n",
    "            except:\n",
    "                continue\n",
    "        M = np.array(M)\n",
    "        v = M.mean(axis=0)\n",
    "        if type(v) != np.ndarray:\n",
    "            return np.zeros(self.dim)\n",
    "        return v # / np.sqrt((v ** 2).sum())\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.apply(self.sentence2vec)\n",
    "        return np.stack(X.values, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = gloveVectorizer(embeddings_dict)\n",
    "X_train = glove.transform(train['comment_text'])\n",
    "X_valid = glove.transform(valid['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation accuracy for toxic comments is 0.89, with precision score of 0.46 and recall score of 0.83\nValidation accuracy for severe_toxic comments is 0.95, with precision score of 0.14 and recall score of 0.87\nValidation accuracy for obscene comments is 0.92, with precision score of 0.38 and recall score of 0.84\nValidation accuracy for threat comments is 0.96, with precision score of 0.05 and recall score of 0.77\nValidation accuracy for insult comments is 0.91, with precision score of 0.35 and recall score of 0.85\nValidation accuracy for identity_hate comments is 0.93, with precision score of 0.10 and recall score of 0.86\n"
    }
   ],
   "source": [
    "## CREATE A MODEL FOR EACH CATEGORY\n",
    "for label in labels:\n",
    "    # Create and fit model\n",
    "    m = LinearSVC(class_weight='balanced')\n",
    "    m.fit(X_train, train[label].values)\n",
    "    # Get predictions\n",
    "    preds = m.predict(X_valid)\n",
    "    # Evaluate predictions\n",
    "    print('Validation accuracy for {0} comments is {1:.2f}, with precision score of {2:.2f} and recall score of {3:.2f}'.format(\n",
    "                                    label, \n",
    "                                    accuracy_score(valid[label], preds), \n",
    "                                    precision_score(valid[label], preds), \n",
    "                                    recall_score(valid[label], preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation accuracy for toxic comments is 0.89, with precision score of 0.45 and recall score of 0.84\nValidation accuracy for severe_toxic comments is 0.88, with precision score of 0.07 and recall score of 0.92\nValidation accuracy for obscene comments is 0.91, with precision score of 0.37 and recall score of 0.85\nValidation accuracy for threat comments is 0.92, with precision score of 0.03 and recall score of 0.84\nValidation accuracy for insult comments is 0.91, with precision score of 0.34 and recall score of 0.85\nValidation accuracy for identity_hate comments is 0.89, with precision score of 0.07 and recall score of 0.89\n"
    }
   ],
   "source": [
    "## CREATE A MODEL FOR EACH CATEGORY\n",
    "for label in labels:\n",
    "    # Create & Fit model\n",
    "    m = LogisticRegression(solver='saga',class_weight='balanced')\n",
    "    m.fit(X_train, train[label])\n",
    "    # Get predictions\n",
    "    preds = m.predict(X_valid)\n",
    "    # Evaluate predictions\n",
    "    print('Validation accuracy for {0} comments is {1:.2f}, with precision score of {2:.2f} and recall score of {3:.2f}'.format(\n",
    "                                    label, \n",
    "                                    accuracy_score(valid[label], preds), \n",
    "                                    precision_score(valid[label], preds), \n",
    "                                    recall_score(valid[label], preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Validation accuracy for toxic comments is 0.93, with precision score of 0.61 and recall score of 0.72\nValidation accuracy for severe_toxic comments is 0.99, with precision score of 0.35 and recall score of 0.42\nValidation accuracy for obscene comments is 0.97, with precision score of 0.68 and recall score of 0.70\nValidation accuracy for threat comments is 1.00, with precision score of 0.58 and recall score of 0.29\nValidation accuracy for insult comments is 0.96, with precision score of 0.58 and recall score of 0.67\nValidation accuracy for identity_hate comments is 0.99, with precision score of 0.41 and recall score of 0.34\n"
    }
   ],
   "source": [
    "## CREATE A MODEL FOR EACH CATEGORY\n",
    "for label in labels:\n",
    "    # Create & Fit model\n",
    "    m = XGBClassifier(n_estimators=100,\n",
    "                      scale_pos_weight= sum(train[label]==0) / sum(train[label]==1),\n",
    "                      n_jobs=-1)\n",
    "    m.fit(X_train, train[label])\n",
    "    # Get predictions\n",
    "    preds = m.predict(X_valid)\n",
    "    # Evaluate predictions\n",
    "    print('Validation accuracy for {0} comments is {1:.2f}, with precision score of {2:.2f} and recall score of {3:.2f}'.format(\n",
    "                                    label, \n",
    "                                    accuracy_score(valid[label], preds), \n",
    "                                    precision_score(valid[label], preds), \n",
    "                                    recall_score(valid[label], preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}