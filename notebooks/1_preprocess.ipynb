{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DATA MANIPULATION\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json\n",
    "\n",
    "## STRING MANIPULATION AND NLP HELP FUNS\n",
    "import re, string, copy\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "## FILE SAVING\n",
    "import pickle\n",
    "\n",
    "## SKLEARN\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/train.csv')             # load data\n",
    "labels = list(data.columns[2:])                     # get labels\n",
    "data['comment_text'].fillna(\"unknown\", inplace=True)# fill empties\n",
    "train, valid = train_test_split(data,               # split into train & test\n",
    "                                random_state=42, \n",
    "                                test_size=0.33, \n",
    "                                shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessor [all credit goes to fizzbuzz from kaggle](https://www.kaggle.com/fizzbuzz/toxic-data-preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lawre\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\lawre\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "class PatternTokenizer(object):\n",
    "    '''Preprocessor credit goes to fizzbuzz from kaggle \n",
    "    (https://www.kaggle.com/fizzbuzz/toxic-data-preprocessing)'''\n",
    "    def __init__(self, lower=True, initial_filters=r\"[^a-z0-9!@#\\$%\\^\\&\\*_\\-,\\.' ]\", re_path='../data/re_patterns.json',\n",
    "                 remove_repetitions=True):\n",
    "        self.lower = lower\n",
    "        self.re_path = re_path\n",
    "        self.initial_filters = initial_filters\n",
    "        self.remove_repetitions = remove_repetitions\n",
    "        self.patterns = None\n",
    "        \n",
    "    def process_text(self, text):\n",
    "        f = open(self.re_path, 'r')\n",
    "        self.patterns = json.load(f)\n",
    "        x = self._preprocess(text)\n",
    "        for target, patterns in self.patterns.items():\n",
    "            for pat in patterns:\n",
    "                x = re.sub(pat, target, x)\n",
    "        x = re.sub(r\"[^a-z' ]\", ' ', x)\n",
    "        return x.split()\n",
    "\n",
    "    def process_ds(self, ds):\n",
    "        ### ds = Data series\n",
    "        f = open(self.re_path, 'r')\n",
    "        self.patterns = json.load(f)\n",
    "        # lower\n",
    "        ds = copy.deepcopy(ds)\n",
    "        if self.lower:\n",
    "            ds = ds.str.lower()\n",
    "        # remove special chars\n",
    "        if self.initial_filters is not None:\n",
    "            ds = ds.str.replace(self.initial_filters, ' ')\n",
    "        # fuuuuck => fuck\n",
    "        if self.remove_repetitions:\n",
    "            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL) \n",
    "            ds = ds.str.replace(pattern, r\"\\1\")\n",
    "\n",
    "        for target, patterns in self.patterns.items():\n",
    "            for pat in patterns:\n",
    "                ds = ds.str.replace(pat, target)\n",
    "\n",
    "        ds = ds.str.replace(r\"[^a-z' ]\", ' ')\n",
    "\n",
    "        return ds.str.split()\n",
    "\n",
    "    def _preprocess(self, text):\n",
    "        # lower\n",
    "        if self.lower:\n",
    "            text = text.lower()\n",
    "\n",
    "        # remove special chars\n",
    "        if self.initial_filters is not None:\n",
    "            text = re.sub(self.initial_filters, ' ', text)\n",
    "\n",
    "        # fuuuuck => fuck\n",
    "        if self.remove_repetitions:\n",
    "            pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "            text = pattern.sub(r\"\\1\", text)\n",
    "        return text\n",
    "        \n",
    "\n",
    "tokenizer = PatternTokenizer()\n",
    "train[\"comment_text\"] = tokenizer.process_ds(train[\"comment_text\"]).str.join(sep=\" \")\n",
    "valid[\"comment_text\"] = tokenizer.process_ds(valid[\"comment_text\"]).str.join(sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29614     sockpuppetry case you have been accused of soc...\n",
       "109036    i've read the archives and various national an...\n",
       "110790    wikipedia is an encyclopedia yes lyrics which ...\n",
       "80583     such as when you mention azeris are geneticall...\n",
       "30047     werdna's rfa hi i'm still slightly wet behind ...\n",
       "                                ...                        \n",
       "119879    redirect talk john loveday experimental physicist\n",
       "103694     back it up post the line here with the reference\n",
       "131932    i won't stop that sometimes germanic equals ge...\n",
       "146867    british bands i think you've mistaken scottish...\n",
       "121958    you are wrong justin thompson is mentioned in ...\n",
       "Name: comment_text, Length: 106912, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text\n",
    "- no longer used\n",
    "- now using pattern tokenizer above by Fizzbuzz from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SKLEARN COMPATIBLE TEXT PREPROCESSOR \n",
    "class preprocessComment(object):\n",
    "    def __init__(self):\n",
    "        # NLTK helper functions\n",
    "        self.stop_words = nltk.corpus.stopwords.words('english')\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "        self.tokenizer = nltk.word_tokenize\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    \n",
    "    def preprocess(self, s):\n",
    "        '''\n",
    "        Input:\n",
    "        Sentence string\n",
    "\n",
    "        Transformations:\n",
    "        Lower case -> Remove stop words ->\n",
    "        Remove non-words -> Lemmatize -> \n",
    "\n",
    "        Output:\n",
    "        Preprocessed sentence string\n",
    "        '''\n",
    "        words = str(s).lower()\n",
    "        words = self.tokenizer(words)\n",
    "        words = [w for w in words if not w in self.stop_words]\n",
    "        words = [w for w in words if w.isalpha()]\n",
    "        words = [self.lemmatizer.lemmatize(w) for w in words]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X.apply(self.preprocess)\n",
    "\n",
    "## PREPROCESS DATA\n",
    "# proc = preprocessComment()\n",
    "# train['comment_text'] = proc.transform(train['comment_text'])\n",
    "# valid['comment_text'] = proc.transform(valid['comment_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/preprocessed.pkl','wb')\n",
    "pickle.dump((train,valid),f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
