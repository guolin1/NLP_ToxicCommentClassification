{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600310892987",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import tensorflow as tf\n",
    "\n",
    "## Save and load\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "### Load preprocessed data & combine labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../data/preprocessed.pkl','rb')\n",
    "train, valid = pickle.load(f)\n",
    "labels = train.columns[2:]\n",
    "ys_train = train[labels]\n",
    "ys_valid = valid[labels]\n",
    "## COMBINE TOXIC CATEGORIES\n",
    "y_train = ys_train.sum(axis=1)\n",
    "y_valid = ys_valid.sum(axis=1)\n",
    "y_train.loc[y_train>1] = 1\n",
    "y_valid.loc[y_valid>1] = 1"
   ]
  },
  {
   "source": [
    "### Get embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Extracted 400000 word vectors\n"
    }
   ],
   "source": [
    "## BUILD GLOVE EMBEDDINGS DICTIONARY\n",
    "embeddings_dict = dict()\n",
    "f = open(r'../data/glove.6B.300d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_dict[word] = vec\n",
    "f.close()\n",
    "print('Extracted {} word vectors'.format(len(embeddings_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Additional params\n",
    "embed_size = 300 # length of vector representation for each word\n",
    "max_features = 50000 # # of unique words to use (number of rows in embedding vector)\n",
    "max_len = 100 # # of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into list of strings\n",
    "train_list = list(train['comment_text'].values)\n",
    "valid_list = list(valid['comment_text'].values)\n",
    "# Create and fit tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_list)\n",
    "# Tokenize train and valid\n",
    "train_tokenized = tokenizer.texts_to_sequences(train_list)\n",
    "valid_tokenized = tokenizer.texts_to_sequences(valid_list)\n",
    "# Padd\n",
    "X_train = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "X_valid = pad_sequences(valid_tokenized, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get mean and std from Glove vectors\n",
    "glove_mean, glove_std = np.stack(embeddings_dict.values()).mean(), np.stack(embeddings_dict.values()).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordidx_dict = tokenizer.word_index # get word indices\n",
    "num_words = min(len(wordidx_dict), max_features) # should be max 50000, but could be less if wordidx dict contains <50000 words\n",
    "# random initliaization of weights\n",
    "embedding_matrix = np.random.normal(glove_mean, \n",
    "                                    glove_std, \n",
    "                                    size = (num_words, embed_size)) \n",
    "# update embedding matrix with glove vectors\n",
    "for word, idx in wordidx_dict.items():\n",
    "    if idx < max_features: # stay within max # of features\n",
    "        # grab glove vector if exists\n",
    "        vec = embeddings_dict.get(word) \n",
    "        # if glove vector exists, add to embedding matrix (i.e., replace random initialization)\n",
    "        if vec is not None: embedding_matrix[idx] = vec "
   ]
  },
  {
   "source": [
    "### Build model and fit"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(max_len,))\n",
    "x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(input) # embedding layer to obtain vectors for words\n",
    "x = Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x) # bidirectional lstm layer\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x) # 1st FC layer\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x) # Output label (6 outputs, 1 for each class for multi-label classification)\n",
    "model = Model(inputs=input, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/2\n3007/3007 [==============================] - 1116s 371ms/step - loss: 0.1195 - accuracy: 0.9568 - val_loss: 0.1045 - val_accuracy: 0.9617\nEpoch 2/2\n3007/3007 [==============================] - 1153s 383ms/step - loss: 0.0945 - accuracy: 0.9646 - val_loss: 0.1024 - val_accuracy: 0.9630\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x17627d40108>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=32, \n",
    "          epochs=2,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\lawre\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: ../artifacts/simple/glove_lstm\\assets\n"
    }
   ],
   "source": [
    "## Save model\n",
    "model.save('../artifacts/simple/glove_lstm')"
   ]
  },
  {
   "source": [
    "### Eval model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('../artifacts/simple/glove_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get predictions\n",
    "preds = model.predict(X_valid, batch_size=32)\n",
    "preds_df = pd.DataFrame(data=preds, columns=['glove_gru'])\n",
    "preds_df.to_csv('../artifacts/simple/preds/glove_lstm.csv')\n",
    "preds_t = preds\n",
    "preds_t[preds<=0.5] = 0\n",
    "preds_t[preds>0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Results for Toxic_Combined comments: Accuracy - 0.96; Precision - 0.91; Recall - 0.72; F1 - 0.80\n"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Label','Accuracy', 'Recall', 'Precision', 'F1', 'Vectorizer', 'model'])\n",
    "\n",
    "## Print results\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "i_preds = preds_t\n",
    "i_true = y_valid\n",
    "\n",
    "# Evaluate predictions\n",
    "acc, prec, recall, f1 = (accuracy_score(i_true, i_preds), \n",
    "                        precision_score(i_true, i_preds), \n",
    "                        recall_score(i_true, i_preds), \n",
    "                        f1_score(i_true, i_preds))\n",
    "\n",
    "# Save results to dataframe\n",
    "results = results.append({'Label': 'Toxic_Combined',\n",
    "                        'Accuracy':acc,\n",
    "                        'Recall':recall,\n",
    "                        'Precision':prec,\n",
    "                        'F1':f1,\n",
    "                        'Vectorizer':'glove',\n",
    "                        'model': 'lstm'}, \n",
    "                        ignore_index = True)\n",
    "\n",
    "# print results\n",
    "print('Results for {0} comments: Accuracy - {1:.2f}; Precision - {2:.2f}; Recall - {3:.2f}; F1 - {4:.2f}'.format(\n",
    "                                'Toxic_Combined', \n",
    "                                acc, \n",
    "                                prec, \n",
    "                                recall,\n",
    "                                f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}