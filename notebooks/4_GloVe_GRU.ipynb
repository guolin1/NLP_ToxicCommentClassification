{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600113037131",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, GRU, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "## Save and load\n",
    "import pickle"
   ]
  },
  {
   "source": [
    "### Load data and GloVe vectors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train & valid splits\n",
    "f = open('../data/preprocessed.pkl','rb')\n",
    "train, valid = pickle.load(f)\n",
    "labels = train.columns[2:]\n",
    "# Get Ys\n",
    "y_train = train[labels].values\n",
    "y_valid = valid[labels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Extracted 400000 word vectors\n"
    }
   ],
   "source": [
    "## BUILD GLOVE EMBEDDINGS DICTIONARY\n",
    "embeddings_dict = dict()\n",
    "f = open(r'../data/glove.6B.300d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vec = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_dict[word] = vec\n",
    "f.close()\n",
    "print('Extracted {} word vectors'.format(len(embeddings_dict)))\n",
    "\n",
    "## Additional params\n",
    "embed_size = len(embeddings_dict[next(iter(embeddings_dict))]) # length of vector representation for each word\n",
    "max_features = 50000 # # of unique words to use (number of rows in embedding vector)\n",
    "max_len = 100 # # of words in a comment to use"
   ]
  },
  {
   "source": [
    "### Standard Keras Preprocessing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into list of strings\n",
    "train_list = list(train['comment_text'].values)\n",
    "valid_list = list(valid['comment_text'].values)\n",
    "# Create and fit tokenizer\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(train_list)\n",
    "# Tokenize train and valid\n",
    "train_tokenized = tokenizer.texts_to_sequences(train_list)\n",
    "valid_tokenized = tokenizer.texts_to_sequences(valid_list)\n",
    "# Padd\n",
    "X_train = pad_sequences(train_tokenized, maxlen=max_len)\n",
    "X_valid = pad_sequences(valid_tokenized, maxlen=max_len)"
   ]
  },
  {
   "source": [
    "### Use GloVe vectors to create embedding matrix. If words are not in GloVe dictionary, then use random initialization. To generate random initialization, use the same mean and std as GloVe vectors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get mean and std from Glove vectors\n",
    "glove_mean, glove_std = np.stack(embeddings_dict.values()).mean(), np.stack(embeddings_dict.values()).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordidx_dict = tokenizer.word_index # get word indices\n",
    "num_words = min(len(wordidx_dict), max_features) # should be max 50000, but could be less if wordidx dict contains <50000 words\n",
    "# random initliaization of weights\n",
    "embedding_matrix = np.random.normal(glove_mean, \n",
    "                                    glove_std, \n",
    "                                    size = (num_words, embed_size)) \n",
    "# update embedding matrix with glove vectors\n",
    "for word, idx in wordidx_dict.items():\n",
    "    if idx < max_features: # stay within max # of features\n",
    "        # grab glove vector if exists\n",
    "        vec = embeddings_dict.get(word) \n",
    "        # if glove vector exists, add to embedding matrix (i.e., replace random initialization)\n",
    "        if vec is not None: embedding_matrix[idx] = vec "
   ]
  },
  {
   "source": [
    "### Build RNN\n",
    "- 1 bidirectional GRU layer with 2 FC layers and dropouts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Input(shape=(max_len,))\n",
    "x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(input) # embedding layer to obtain vectors for words\n",
    "x = Bidirectional(GRU(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x) # bidirectional lstm layer\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation=\"relu\")(x) # 1st FC layer\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x) # Output label (6 outputs, 1 for each class for multi-label classification)\n",
    "model = Model(inputs=input, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "source": [
    "### Fit Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/2\n3007/3007 [==============================] - 695s 231ms/step - loss: 0.0576 - accuracy: 0.9173 - val_loss: 0.0488 - val_accuracy: 0.9943\nEpoch 2/2\n3007/3007 [==============================] - 730s 243ms/step - loss: 0.0446 - accuracy: 0.9787 - val_loss: 0.0483 - val_accuracy: 0.9914\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1482892a448>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "model.fit(X_train, \n",
    "          y_train, \n",
    "          batch_size=32, \n",
    "          epochs=2,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From C:\\Users\\lawre\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: ../artifacts/glove_gru\\assets\n"
    }
   ],
   "source": [
    "## Save model\n",
    "model.save('../artifacts/glove_gru')"
   ]
  },
  {
   "source": [
    "### Check performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get predictions\n",
    "preds = model.predict(X_valid, batch_size=32)\n",
    "preds_t = preds\n",
    "preds_t[preds<=0.5] = 0\n",
    "preds_t[preds>0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Results for toxic comments: Accuracy - 0.96; Precision - 0.87; Recall - 0.72; F1 - 0.79\nResults for severe_toxic comments: Accuracy - 0.99; Precision - 0.72; Recall - 0.06; F1 - 0.12\nResults for obscene comments: Accuracy - 0.98; Precision - 0.87; Recall - 0.75; F1 - 0.81\nResults for threat comments: Accuracy - 1.00; Precision - 0.25; Recall - 0.01; F1 - 0.01\nResults for insult comments: Accuracy - 0.97; Precision - 0.79; Recall - 0.66; F1 - 0.72\nResults for identity_hate comments: Accuracy - 0.99; Precision - 0.69; Recall - 0.28; F1 - 0.40\n"
    }
   ],
   "source": [
    "## Print results\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "for i in range(preds_t.shape[1]):\n",
    "    i_preds = preds[:,i]\n",
    "    i_true = y_valid[:,i]\n",
    "    label = labels[i]\n",
    "    print('Results for {0} comments: Accuracy - {1:.2f}; Precision - {2:.2f}; Recall - {3:.2f}; F1 - {4:.2f}'.format(\n",
    "                                    label, \n",
    "                                    accuracy_score(i_true, i_preds), \n",
    "                                    precision_score(i_true, i_preds), \n",
    "                                    recall_score(i_true, i_preds),\n",
    "                                    f1_score(i_true, i_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}